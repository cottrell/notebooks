{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See here: https://github.com/fchollet/keras/issues/1061"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(135, 6) (135, 1) (15, 6) (15, 1)\n"
     ]
    }
   ],
   "source": [
    "# %load gmm.py\n",
    "\"\"\"\n",
    "See here: https://github.com/fchollet/keras/issues/1061\n",
    "\"\"\"\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Layer\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "### load data\n",
    "from imp import reload\n",
    "import site; site.addsitedir('..')\n",
    "import mylib.data as md\n",
    "reload(md)\n",
    "\n",
    "df, dfd = md.iris()\n",
    "\n",
    "y_cols = ['petal length (cm)']\n",
    "x_cols = [x for x in dfd.columns if x not in y_cols]\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfd[x_cols].values,\n",
    "        dfd[y_cols].values, test_size=0.1)\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Layer\n",
    "from keras.optimizers import SGD\n",
    "from theano import tensor as T\n",
    "import theano\n",
    "\n",
    "class GMMActivation(Layer):\n",
    "    \"\"\"\n",
    "    GMM-like activation function.\n",
    "    Assumes that input has (D+2)*M dimensions, where D is the dimensionality of the\n",
    "    target data. The first M*D features are treated as means, the next M features as\n",
    "    standard devs and the last M features as mixture components of the GMM.\n",
    "    \"\"\"\n",
    "    def __init__(self, M, D, **kwargs):\n",
    "        super(GMMActivation, self).__init__(**kwargs)\n",
    "        self.M = M\n",
    "        self.D = D\n",
    "\n",
    "    def get_output(self, train=False):\n",
    "      X = self.get_input(train)\n",
    "      # D = T.shape(X)[1]/self.M - 2\n",
    "      # leave mu values as they are since they're unconstrained\n",
    "      # scale sigmas with exp, s.t. all values are non-negative\n",
    "      X = T.set_subtensor(X[:,D*self.M:(D+1)*self.M], T.exp(X[:,D*self.M:(D+1)*self.M]))\n",
    "      # scale alphas with softmax, s.t. that all values are between [0,1] and sum up to 1\n",
    "      X = T.set_subtensor(X[:,(D+1)*self.M:(D+2)*self.M], T.nnet.softmax(X[:,(D+1)*self.M:(D+2)*self.M]))\n",
    "      return X\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\"name\": self.__class__.__name__,\n",
    "                  \"M\": self.M}\n",
    "        base_config = super(GMMActivation, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "def gmm_loss_factory(M, D):\n",
    "    def gmm_loss(y_true, y_pred):\n",
    "      \"\"\"\n",
    "      GMM loss function.\n",
    "      Assumes that y_pred has (D+2)*M dimensions and y_true has D dimensions. The first\n",
    "      M*D features are treated as means, the next M features as standard devs and the last\n",
    "      M features as mixture components of the GMM.\n",
    "      \"\"\"\n",
    "      def loss(m, M, D, y_true, y_pred):\n",
    "        mu = y_pred[:,D*m:(m+1)*D]\n",
    "        sigma = y_pred[:,D*M+m]\n",
    "        alpha = y_pred[:,(D+1)*M+m]\n",
    "        return (alpha/sigma) * T.exp(-T.sum(T.sqr(mu-y_true),-1)/(2*sigma**2))\n",
    "\n",
    "      # D = T.shape(y_true)[1]\n",
    "      # M = T.shape(y_pred)[1]/(D+2)\n",
    "      seq = T.arange(M)\n",
    "      result, _ = theano.scan(fn=loss, outputs_info=None,\n",
    "        sequences=seq, non_sequences=[M, D, y_true, y_pred])\n",
    "      return -T.log(result.sum(0))\n",
    "    return gmm_loss\n",
    "\n",
    "M = 3\n",
    "D = y_train.shape[1]\n",
    "gmm_loss = gmm_loss_factory(M, D)\n",
    "model_gmm = Sequential()\n",
    "model_gmm.add(Dense(input_dim=input_dim, output_dim=(input_dim + 2) * M))\n",
    "model_gmm.add(GMMActivation(M, D))\n",
    "\n",
    "sgd = SGD(lr=0.0025, decay=1e-6, momentum=0.5, nesterov=True)\n",
    "\n",
    "# model_gmm.compile(loss=gmm_loss, optimizer='adam')\n",
    "model_gmm.compile(loss=gmm_loss, optimizer=sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((135, 6), (135, 1))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 135 samples, validate on 15 samples\n",
      "Epoch 1/100\n",
      "135/135 [==============================] - 0s - loss: 1.9835 - acc: 0.3037 - val_loss: 1.4745 - val_acc: 0.4000\n",
      "Epoch 2/100\n",
      "135/135 [==============================] - 0s - loss: 1.7667 - acc: 0.4148 - val_loss: 1.4519 - val_acc: 0.4000\n",
      "Epoch 3/100\n",
      "135/135 [==============================] - 0s - loss: 1.7011 - acc: 0.4593 - val_loss: 2.0939 - val_acc: 0.6000\n",
      "Epoch 4/100\n",
      "135/135 [==============================] - 0s - loss: 1.9374 - acc: 0.8444 - val_loss: 2.0297 - val_acc: 0.8667\n",
      "Epoch 5/100\n",
      "135/135 [==============================] - 0s - loss: 1.9580 - acc: 0.9407 - val_loss: 1.9354 - val_acc: 1.0000\n",
      "Epoch 6/100\n",
      "135/135 [==============================] - 0s - loss: 1.8643 - acc: 0.9926 - val_loss: 1.8438 - val_acc: 1.0000\n",
      "Epoch 7/100\n",
      "135/135 [==============================] - 0s - loss: 1.7716 - acc: 1.0000 - val_loss: 1.7509 - val_acc: 1.0000\n",
      "Epoch 8/100\n",
      "135/135 [==============================] - 0s - loss: 1.6768 - acc: 1.0000 - val_loss: 1.6533 - val_acc: 1.0000\n",
      "Epoch 9/100\n",
      "135/135 [==============================] - 0s - loss: 1.5767 - acc: 1.0000 - val_loss: 1.5530 - val_acc: 1.0000\n",
      "Epoch 10/100\n",
      "135/135 [==============================] - 0s - loss: 1.4771 - acc: 1.0000 - val_loss: 1.4485 - val_acc: 1.0000\n",
      "Epoch 11/100\n",
      "135/135 [==============================] - 0s - loss: 1.3700 - acc: 1.0000 - val_loss: 1.3497 - val_acc: 1.0000\n",
      "Epoch 12/100\n",
      "135/135 [==============================] - 0s - loss: 1.2723 - acc: 1.0000 - val_loss: 1.2639 - val_acc: 1.0000\n",
      "Epoch 13/100\n",
      "135/135 [==============================] - 0s - loss: 1.1928 - acc: 1.0000 - val_loss: 1.2087 - val_acc: 1.0000\n",
      "Epoch 14/100\n",
      "135/135 [==============================] - 0s - loss: 1.1371 - acc: 1.0000 - val_loss: 1.1572 - val_acc: 1.0000\n",
      "Epoch 15/100\n",
      "135/135 [==============================] - 0s - loss: 1.0985 - acc: 1.0000 - val_loss: 1.1300 - val_acc: 1.0000\n",
      "Epoch 16/100\n",
      "135/135 [==============================] - 0s - loss: 1.0732 - acc: 1.0000 - val_loss: 1.1110 - val_acc: 1.0000\n",
      "Epoch 17/100\n",
      "135/135 [==============================] - 0s - loss: 1.0542 - acc: 1.0000 - val_loss: 1.0940 - val_acc: 1.0000\n",
      "Epoch 18/100\n",
      "135/135 [==============================] - 0s - loss: 1.0378 - acc: 1.0000 - val_loss: 1.0775 - val_acc: 1.0000\n",
      "Epoch 19/100\n",
      "135/135 [==============================] - 0s - loss: 1.0224 - acc: 1.0000 - val_loss: 1.0629 - val_acc: 1.0000\n",
      "Epoch 20/100\n",
      "135/135 [==============================] - 0s - loss: 1.0093 - acc: 1.0000 - val_loss: 1.0473 - val_acc: 1.0000\n",
      "Epoch 21/100\n",
      "135/135 [==============================] - 0s - loss: 0.9937 - acc: 1.0000 - val_loss: 1.0351 - val_acc: 1.0000\n",
      "Epoch 22/100\n",
      "135/135 [==============================] - 0s - loss: 0.9785 - acc: 1.0000 - val_loss: 1.0182 - val_acc: 1.0000\n",
      "Epoch 23/100\n",
      "135/135 [==============================] - 0s - loss: 0.9648 - acc: 1.0000 - val_loss: 1.0066 - val_acc: 1.0000\n",
      "Epoch 24/100\n",
      "135/135 [==============================] - 0s - loss: 0.9490 - acc: 1.0000 - val_loss: 0.9873 - val_acc: 1.0000\n",
      "Epoch 25/100\n",
      "135/135 [==============================] - 0s - loss: 0.9324 - acc: 1.0000 - val_loss: 0.9706 - val_acc: 1.0000\n",
      "Epoch 26/100\n",
      "135/135 [==============================] - 0s - loss: 0.9150 - acc: 1.0000 - val_loss: 0.9548 - val_acc: 1.0000\n",
      "Epoch 27/100\n",
      "135/135 [==============================] - 0s - loss: 0.9012 - acc: 1.0000 - val_loss: 0.9383 - val_acc: 1.0000\n",
      "Epoch 28/100\n",
      "135/135 [==============================] - 0s - loss: 0.8863 - acc: 1.0000 - val_loss: 0.9224 - val_acc: 1.0000\n",
      "Epoch 29/100\n",
      "135/135 [==============================] - 0s - loss: 0.8662 - acc: 1.0000 - val_loss: 0.9054 - val_acc: 1.0000\n",
      "Epoch 30/100\n",
      "135/135 [==============================] - 0s - loss: 0.8504 - acc: 1.0000 - val_loss: 0.8844 - val_acc: 1.0000\n",
      "Epoch 31/100\n",
      "135/135 [==============================] - 0s - loss: 0.8306 - acc: 1.0000 - val_loss: 0.8651 - val_acc: 1.0000\n",
      "Epoch 32/100\n",
      "135/135 [==============================] - 0s - loss: 0.8122 - acc: 1.0000 - val_loss: 0.8486 - val_acc: 1.0000\n",
      "Epoch 33/100\n",
      "135/135 [==============================] - 0s - loss: 0.7962 - acc: 1.0000 - val_loss: 0.8287 - val_acc: 1.0000\n",
      "Epoch 34/100\n",
      "135/135 [==============================] - 0s - loss: 0.7787 - acc: 1.0000 - val_loss: 0.8073 - val_acc: 1.0000\n",
      "Epoch 35/100\n",
      "135/135 [==============================] - 0s - loss: 0.7616 - acc: 1.0000 - val_loss: 0.7884 - val_acc: 1.0000\n",
      "Epoch 36/100\n",
      "135/135 [==============================] - 0s - loss: 0.7326 - acc: 1.0000 - val_loss: 0.7724 - val_acc: 1.0000\n",
      "Epoch 37/100\n",
      "135/135 [==============================] - 0s - loss: 0.7142 - acc: 1.0000 - val_loss: 0.7440 - val_acc: 1.0000\n",
      "Epoch 38/100\n",
      "135/135 [==============================] - 0s - loss: 0.6898 - acc: 1.0000 - val_loss: 0.7209 - val_acc: 1.0000\n",
      "Epoch 39/100\n",
      "135/135 [==============================] - 0s - loss: 0.6664 - acc: 1.0000 - val_loss: 0.6961 - val_acc: 1.0000\n",
      "Epoch 40/100\n",
      "135/135 [==============================] - 0s - loss: 0.6443 - acc: 1.0000 - val_loss: 0.6740 - val_acc: 1.0000\n",
      "Epoch 41/100\n",
      "135/135 [==============================] - 0s - loss: 0.6223 - acc: 1.0000 - val_loss: 0.6453 - val_acc: 1.0000\n",
      "Epoch 42/100\n",
      "135/135 [==============================] - 0s - loss: 0.5891 - acc: 1.0000 - val_loss: 0.6202 - val_acc: 1.0000\n",
      "Epoch 43/100\n",
      "135/135 [==============================] - 0s - loss: 0.5632 - acc: 1.0000 - val_loss: 0.5876 - val_acc: 1.0000\n",
      "Epoch 44/100\n",
      "135/135 [==============================] - 0s - loss: 0.5356 - acc: 1.0000 - val_loss: 0.5583 - val_acc: 1.0000\n",
      "Epoch 45/100\n",
      "135/135 [==============================] - 0s - loss: 0.5049 - acc: 1.0000 - val_loss: 0.5270 - val_acc: 1.0000\n",
      "Epoch 46/100\n",
      "135/135 [==============================] - 0s - loss: 0.4769 - acc: 1.0000 - val_loss: 0.4980 - val_acc: 1.0000\n",
      "Epoch 47/100\n",
      "135/135 [==============================] - 0s - loss: 0.4455 - acc: 1.0000 - val_loss: 0.4603 - val_acc: 1.0000\n",
      "Epoch 48/100\n",
      "135/135 [==============================] - 0s - loss: 0.4028 - acc: 1.0000 - val_loss: 0.4293 - val_acc: 1.0000\n",
      "Epoch 49/100\n",
      "135/135 [==============================] - 0s - loss: 0.3670 - acc: 1.0000 - val_loss: 0.3839 - val_acc: 1.0000\n",
      "Epoch 50/100\n",
      "135/135 [==============================] - 0s - loss: 0.3259 - acc: 1.0000 - val_loss: 0.3335 - val_acc: 1.0000\n",
      "Epoch 51/100\n",
      "135/135 [==============================] - 0s - loss: 0.2823 - acc: 1.0000 - val_loss: 0.2827 - val_acc: 1.0000\n",
      "Epoch 52/100\n",
      "135/135 [==============================] - 0s - loss: 0.2364 - acc: 1.0000 - val_loss: 0.2369 - val_acc: 1.0000\n",
      "Epoch 53/100\n",
      "135/135 [==============================] - 0s - loss: 0.1884 - acc: 1.0000 - val_loss: 0.1798 - val_acc: 1.0000\n",
      "Epoch 54/100\n",
      "135/135 [==============================] - 0s - loss: 0.1305 - acc: 1.0000 - val_loss: 0.1133 - val_acc: 1.0000\n",
      "Epoch 55/100\n",
      "135/135 [==============================] - 0s - loss: 0.0807 - acc: 1.0000 - val_loss: 0.0475 - val_acc: 1.0000\n",
      "Epoch 56/100\n",
      "135/135 [==============================] - 0s - loss: 0.0149 - acc: 1.0000 - val_loss: -0.0178 - val_acc: 1.0000\n",
      "Epoch 57/100\n",
      "135/135 [==============================] - 0s - loss: -0.0426 - acc: 1.0000 - val_loss: -0.0926 - val_acc: 1.0000\n",
      "Epoch 58/100\n",
      "135/135 [==============================] - 0s - loss: -0.0931 - acc: 1.0000 - val_loss: -0.1829 - val_acc: 1.0000\n",
      "Epoch 59/100\n",
      "135/135 [==============================] - 0s - loss: -0.1720 - acc: 1.0000 - val_loss: -0.2705 - val_acc: 1.0000\n",
      "Epoch 60/100\n",
      "135/135 [==============================] - 0s - loss: -0.2457 - acc: 1.0000 - val_loss: -0.1558 - val_acc: 1.0000\n",
      "Epoch 61/100\n",
      "135/135 [==============================] - 0s - loss: -0.2528 - acc: 1.0000 - val_loss: -0.4380 - val_acc: 1.0000\n",
      "Epoch 62/100\n",
      "135/135 [==============================] - 0s - loss: -0.3809 - acc: 1.0000 - val_loss: -0.4895 - val_acc: 1.0000\n",
      "Epoch 63/100\n",
      "135/135 [==============================] - 0s - loss: -0.4288 - acc: 1.0000 - val_loss: -0.6160 - val_acc: 1.0000\n",
      "Epoch 64/100\n",
      "135/135 [==============================] - 0s - loss: -0.4730 - acc: 1.0000 - val_loss: -0.6625 - val_acc: 1.0000\n",
      "Epoch 65/100\n",
      "135/135 [==============================] - 0s - loss: -0.5214 - acc: 1.0000 - val_loss: 0.1176 - val_acc: 0.9333\n",
      "Epoch 66/100\n",
      "135/135 [==============================] - 0s - loss: -0.1818 - acc: 1.0000 - val_loss: -0.6560 - val_acc: 1.0000\n",
      "Epoch 67/100\n",
      "135/135 [==============================] - 0s - loss: -0.5646 - acc: 1.0000 - val_loss: -0.7109 - val_acc: 1.0000\n",
      "Epoch 68/100\n",
      "135/135 [==============================] - 0s - loss: -0.5762 - acc: 1.0000 - val_loss: -0.6828 - val_acc: 1.0000\n",
      "Epoch 69/100\n",
      "135/135 [==============================] - 0s - loss: -0.5654 - acc: 1.0000 - val_loss: -0.6212 - val_acc: 1.0000\n",
      "Epoch 70/100\n",
      "135/135 [==============================] - 0s - loss: -0.5428 - acc: 1.0000 - val_loss: -0.3827 - val_acc: 1.0000\n",
      "Epoch 71/100\n",
      "135/135 [==============================] - 0s - loss: -0.5680 - acc: 1.0000 - val_loss: -0.8917 - val_acc: 1.0000\n",
      "Epoch 72/100\n",
      "135/135 [==============================] - 0s - loss: -0.6247 - acc: 1.0000 - val_loss: -0.5764 - val_acc: 1.0000\n",
      "Epoch 73/100\n",
      "135/135 [==============================] - 0s - loss: -0.4794 - acc: 1.0000 - val_loss: -0.7808 - val_acc: 1.0000\n",
      "Epoch 74/100\n",
      "135/135 [==============================] - 0s - loss: -0.5942 - acc: 1.0000 - val_loss: -0.0362 - val_acc: 1.0000\n",
      "Epoch 75/100\n",
      "135/135 [==============================] - 0s - loss: -0.4945 - acc: 1.0000 - val_loss: -0.8440 - val_acc: 1.0000\n",
      "Epoch 76/100\n",
      "135/135 [==============================] - 0s - loss: -0.5995 - acc: 1.0000 - val_loss: -0.6655 - val_acc: 1.0000\n",
      "Epoch 77/100\n",
      "135/135 [==============================] - 0s - loss: -0.5794 - acc: 1.0000 - val_loss: -0.6529 - val_acc: 1.0000\n",
      "Epoch 78/100\n",
      "135/135 [==============================] - 0s - loss: -0.5180 - acc: 1.0000 - val_loss: -0.7475 - val_acc: 1.0000\n",
      "Epoch 79/100\n",
      "135/135 [==============================] - 0s - loss: -0.6012 - acc: 1.0000 - val_loss: -0.8965 - val_acc: 1.0000\n",
      "Epoch 80/100\n",
      "135/135 [==============================] - 0s - loss: -0.6230 - acc: 1.0000 - val_loss: -0.0704 - val_acc: 0.9333\n",
      "Epoch 81/100\n",
      "135/135 [==============================] - 0s - loss: -0.4382 - acc: 1.0000 - val_loss: -0.8298 - val_acc: 1.0000\n",
      "Epoch 82/100\n",
      "135/135 [==============================] - 0s - loss: -0.5177 - acc: 1.0000 - val_loss: -0.8806 - val_acc: 1.0000\n",
      "Epoch 83/100\n",
      "135/135 [==============================] - 0s - loss: -0.6202 - acc: 1.0000 - val_loss: -0.9314 - val_acc: 1.0000\n",
      "Epoch 84/100\n",
      "135/135 [==============================] - 0s - loss: -0.6692 - acc: 1.0000 - val_loss: -0.9430 - val_acc: 1.0000\n",
      "Epoch 85/100\n",
      "135/135 [==============================] - 0s - loss: -0.6112 - acc: 1.0000 - val_loss: -0.6383 - val_acc: 1.0000\n",
      "Epoch 86/100\n",
      "135/135 [==============================] - 0s - loss: -0.4826 - acc: 1.0000 - val_loss: -0.4827 - val_acc: 1.0000\n",
      "Epoch 87/100\n",
      "135/135 [==============================] - 0s - loss: -0.5294 - acc: 1.0000 - val_loss: -0.8877 - val_acc: 1.0000\n",
      "Epoch 88/100\n",
      "135/135 [==============================] - 0s - loss: -0.6809 - acc: 1.0000 - val_loss: -0.5501 - val_acc: 1.0000\n",
      "Epoch 89/100\n",
      "135/135 [==============================] - 0s - loss: -0.6233 - acc: 1.0000 - val_loss: -0.7659 - val_acc: 1.0000\n",
      "Epoch 90/100\n",
      "135/135 [==============================] - 0s - loss: -0.6091 - acc: 1.0000 - val_loss: -0.6958 - val_acc: 1.0000\n",
      "Epoch 91/100\n",
      "135/135 [==============================] - 0s - loss: -0.6216 - acc: 1.0000 - val_loss: -0.9368 - val_acc: 1.0000\n",
      "Epoch 92/100\n",
      "135/135 [==============================] - 0s - loss: -0.5483 - acc: 1.0000 - val_loss: -0.7209 - val_acc: 1.0000\n",
      "Epoch 93/100\n",
      "135/135 [==============================] - 0s - loss: -0.6461 - acc: 1.0000 - val_loss: -0.1397 - val_acc: 1.0000\n",
      "Epoch 94/100\n",
      "135/135 [==============================] - 0s - loss: -0.5311 - acc: 1.0000 - val_loss: -0.8150 - val_acc: 1.0000\n",
      "Epoch 95/100\n",
      "135/135 [==============================] - 0s - loss: -0.6077 - acc: 1.0000 - val_loss: -0.8999 - val_acc: 1.0000\n",
      "Epoch 96/100\n",
      "135/135 [==============================] - 0s - loss: -0.5274 - acc: 1.0000 - val_loss: -0.8103 - val_acc: 1.0000\n",
      "Epoch 97/100\n",
      "135/135 [==============================] - 0s - loss: -0.6957 - acc: 1.0000 - val_loss: -0.8226 - val_acc: 1.0000\n",
      "Epoch 98/100\n",
      "135/135 [==============================] - 0s - loss: -0.6739 - acc: 1.0000 - val_loss: -0.8719 - val_acc: 1.0000\n",
      "Epoch 99/100\n",
      "135/135 [==============================] - 0s - loss: -0.6899 - acc: 1.0000 - val_loss: -0.4418 - val_acc: 1.0000\n",
      "Epoch 100/100\n",
      "135/135 [==============================] - 0s - loss: -0.3805 - acc: 1.0000 - val_loss: -0.8340 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "out = model_gmm.fit(X_train, y_train, nb_epoch=100, batch_size=30, validation_data=(X_test, y_test), show_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAECCAYAAADq7fyyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEZEYo8IuIhJj/j8EbDwPrtV2yQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11e2f97b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "h = pandas.DataFrame(out.history)\n",
    "h[['loss', 'val_loss']].plot(logy=True)\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = model_gmm.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135, 24)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
